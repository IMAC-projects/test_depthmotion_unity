

Constraints:

_ Default depth texture mode in Unity does not render some objects:
	1) whose material's shader has no shader caster pass. (might be an issue)
	2) not opaque (renderqueue > 2500)


see: https://docs.unity3d.com/Manual/class-RenderTexture.html

1
2) Do we write transparent objects into this depth buffer? Yes, I think we should.
The depth compenent is used in the network as a classifier: 
1_ closer objects are upscaled in finer details than farther ones
2_ if an object is expected (wrt to motion vectors) to cross another in camera space, it is the closer one which is rendered. 
Obvious, but the 'crossing' information might not be there at all in a lower-sampled image, so we need to be able to make it appear in the upscaled output.
As such, even if, say, a vase is transparent, if it is close to the camera it should be rendered 

Possible answer: full render pass with replaced shaders.

_ We can choose to render objects according to 'RenderType' tags. 
This tag is set for all default shaders in Unity, however it might be missing in user-created ones. 

see: https://docs.unity3d.com/Manual/SL-ShaderReplacement.html


_ We should disable the Dynamic Scaling option on Render Textures. 
It will cause the texture's resolution to be lowered when GPU-bound.
As such, the texture dimension will be no longer be input-compatible with the network.
In fact,  we could then upcale the texture, but it would defeat the original purpose,
and in our case significantly deteriorate the Networks' upscaling, because it heavily relies on accurate Depth map, Motion Vectors etc.


see: https://docs.unity3d.com/Manual/DynamicResolution.html


_ Same thing about mipmapping the texture -> harmful

Thoughts:

_ Should effects be predicted using the network? Depends, I think some effects are just textures placed in the 3D environment and oriented so that they face the camera, so those we would scale beforehand. 
In any case, I think it's not worth it. it would be more accurate to have a second network trained and specialised in upscaling particle effects. It would however be twice as long to render.

_ We should not predict SDF-rendered text object, they have a built-in scaling thing going on. It's precisely what makes them useful.

_ UI might likewise be predicted by a specialised network. But if we use the same, their depth value will be the max depth value, so that is properly predicted.

